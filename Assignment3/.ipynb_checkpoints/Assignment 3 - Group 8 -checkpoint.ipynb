{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 Group no. 8\n",
    "### Project members: \n",
    "Group NO. 8: YI LI yi9@kth.com CHEN YANG cheyan@kth.com JINRUI LIU jinrui@kth.se\n",
    "\n",
    "### Declaration\n",
    "By submitting this solution, it is hereby declared that all individuals listed above have contributed to the solution, either with code that appear in the final solution below, or with code that has been evaluated and compared to the final solution, but for some reason has been excluded. It is also declared that all project members fully understand all parts of the final solution and can explain it upon request.\n",
    "\n",
    "It is furthermore declared that the code below is a contribution by the project members only, and specifically that no part of the solution has been copied from any other source (except for lecture slides at the course ID2214) and no part of the solution has been provided by someone not listed as project member above.\n",
    "\n",
    "It is furthermore declared that it has been understood that no other library/package than the Python 3 standard library, NumPy, pandas and time may be used in the solution for this assignment.\n",
    "\n",
    "### Instructions\n",
    "All assignments starting with number 1 below are mandatory. Satisfactory solutions\n",
    "will give 1 point (in total). If they in addition are good (all parts work more or less \n",
    "as they should), completed on time (submitted before the deadline in Canvas) and according\n",
    "to the instructions, together with satisfactory solutions of assignments starting with \n",
    "number 2 below, then the assignment will receive 2 points (in total).\n",
    "\n",
    "It is highly recommended that you do not develop the code directly within the notebook\n",
    "but that you copy the comments and test cases to your regular development environment\n",
    "and only when everything works as expected, that you paste your functions into this\n",
    "notebook, do a final testing (all cells should succeed) and submit the whole notebook \n",
    "(a single file) in Canvas (do not forget to fill in your group number and names above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NumPy, pandas and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reused functions from Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste functions from Assignment 1 here that you need for this assignment\n",
    "# Copy and paste functions from Assignment 1 here that you need for this assignment\n",
    "\n",
    "# create_normalization\n",
    "def create_normalization(train_df, normalizationtype = 'zscore'):\n",
    "    dataf = train_df.copy()\n",
    "    # old version\n",
    "    columns = [col for col in list(dataf) if (col != 'ID' and col != 'CLASS')]\n",
    "    columns = [col for col in columns if (dataf[col].dtype in ['float64', 'float32', 'int'])]\n",
    "    # new version\n",
    "    columns = []\n",
    "    for col in list(dataf):\n",
    "        if(col not in ['ID', 'CLASS'] and dataf[col].dtype in ['float64', 'float32', 'int']):\n",
    "            columns.append(col)\n",
    "    \n",
    "    normalization = {}\n",
    "    if normalizationtype == 'zscore':\n",
    "        for i in columns: \n",
    "            mu   = dataf[i].mean()\n",
    "            delta= dataf[i].std()\n",
    "            dataf[i] = (dataf[i] - mu)/delta\n",
    "            normalization[i] = ('zscore',mu,delta) \n",
    "    else:                                              \n",
    "        for i in columns:\n",
    "            maxx = dataf[i].max()\n",
    "            minn = dataf[i].min()\n",
    "            dataf[i] = (dataf[i] - minn)/maxx-minn\n",
    "            normalization[i] = ('minmax',minn,maxx)          \n",
    "    return dataf, normalization\n",
    "\n",
    "# apply_normalization\n",
    "def apply_normalization(test_df,normalization):                        # this function used on test_dataset\n",
    "    dataf = test_df.copy()\n",
    "    columns = [col for col in list(dataf) if (col != 'ID' and col != 'CLASS')]\n",
    "    columns = [col for col in columns if (dataf[col].dtype in ['float64', 'float32', 'int'])]  \n",
    "    #print(columns)\n",
    "    \n",
    "    # new version\n",
    "    columns = []\n",
    "    for col in list(dataf):\n",
    "        if(col not in ['ID', 'CLASS'] and dataf[col].dtype in ['float64', 'float32', 'int']):\n",
    "            columns.append(col)\n",
    "    #print(columns)\n",
    "\n",
    "    for i in columns:\n",
    "        if normalization[i][0] == 'zscore':\n",
    "            #print(normalization[i])\n",
    "            mu   = normalization[i][1]\n",
    "            delta= normalization[i][2]\n",
    "            dataf[i] = (dataf[i] - mu)/delta\n",
    "        else:\n",
    "            minn = normalization[i][1]\n",
    "            maxx = normalization[i][2]\n",
    "            dataf[i] = (dataf[i] - minn)/(maxx-minn)       # limit the elements within [0,1]\n",
    "            dataf[i] = [0 if element < 0 else element for element in dataf[i]]\n",
    "            dataf[i] = [1 if element > 1 else element for element in dataf[i]]\n",
    "    return dataf   \n",
    "\n",
    "# create_imputation\n",
    "def create_imputation(train_df):\n",
    "    dataf = train_df.copy()\n",
    "    columns = [col for col in list(dataf) if (col != 'ID' and col != 'CLASS')]\n",
    "    #print(columns)\n",
    "\n",
    "    imputa = {}\n",
    "    for col in list(dataf):\n",
    "        #print(dataf[col].dtype)\n",
    "        if(col not in ['ID', 'CLASS'] and dataf[col].dtype in ['float64', 'float32', 'int64', 'int32', 'int']):\n",
    "            if pd.isnull(dataf[col]).all():          # check if all elements are NaN, replce by 0\n",
    "                imputa[col] = 0\n",
    "                dataf[col] = dataf[col].fillna(0)    \n",
    "            imputa[col]=(dataf[col].mean()) \n",
    "            dataf[col] = dataf[col].fillna(imputa[col]) \n",
    "            \n",
    "        elif(col not in ['ID', 'CLASS'] and dataf[col].dtype in ['object', 'category']):\n",
    "            if pd.isnull(dataf[col]).all():          # check if all elements are NaN, replace by \"\"\n",
    "                imputa[col] = \"\"\n",
    "                dataf[col] = dataf[col].fillna(\"\") \n",
    "                #dataf[i].fillna(\"\",inplace=True)\n",
    "            imputa[col]=dataf[col].mode()[0] \n",
    "            dataf[col] = dataf[col].fillna(imputa[col]) \n",
    "            #dataf[i].fillna(mode[0],inplace=True)\n",
    "    #dataf['CLASS'].astype('category')\n",
    "    return dataf, imputa\n",
    "\n",
    "# apply_imputation\n",
    "def apply_imputation(df_init, imputa):\n",
    "    dataf = df_init.copy()\n",
    "    for col in list(dataf):\n",
    "        if(col not in ['ID','CLASS']): \n",
    "            imputa_value = imputa[col]\n",
    "            dataf[col] = dataf[col].fillna(imputa_value)    # replace NaN of each colume by corresponding Imputation               \n",
    "    return(dataf)\n",
    "\n",
    "########################################## Below is for Bayes mainly ########################################################\n",
    "\n",
    "def create_bins(input_df,nobins,bintype):\n",
    "    train_df = input_df.copy()\n",
    "    columns = [col for col in list(train_df) if (col != 'ID' and col != 'CLASS')]\n",
    "    columns = [col for col in columns if (train_df[col].dtype in ['float64', 'float32', 'int'])]\n",
    "\n",
    "    # new version\n",
    "    columns = []\n",
    "    for col in list(train_df):\n",
    "        if(col not in ['ID', 'CLASS'] and train_df[col].dtype in ['float64', 'float32', 'int']):\n",
    "            columns.append(col)\n",
    "\n",
    "    bin_dict={}\n",
    "    if bintype == 'equal-size':\n",
    "        for i in columns:\n",
    "            train_df[i], binss= pd.qcut(train_df[i], q=nobins,labels=False, retbins=True, duplicates=\"drop\")\n",
    "            #print(train_df[i].dtypes)\n",
    "            train_df[i] = pd.Categorical(train_df[i])                   # convert int to categorical data\n",
    "            bin_temp = binss.tolist()                                   # convert array into list   \n",
    "            #bin_temp = [round(value,3) for value in bin_temp]          # round to 2 decimal\n",
    "            bin_temp[0]=-np.inf                                         # replace the first and last elements\n",
    "            bin_temp[-1]=np.inf\n",
    "            bin_dict[i]=bin_temp                                        # map into dict\n",
    "    else:\n",
    "        for i in columns:\n",
    "            train_df[i], binss= pd.cut(train_df[i], nobins,labels=False, retbins=True, duplicates=\"drop\")\n",
    "            #print(train_df[i].dtypes)\n",
    "            train_df[i] = pd.Categorical(train_df[i])                   # convert int to categorical data\n",
    "            bin_temp = binss.tolist()                                   # convert array into list   \n",
    "            #bin_temp = [round(value,3) for value in bin_temp]          # round to 2 decimal\n",
    "            bin_temp[0]=-np.inf                                         # replace the first and last elements\n",
    "            bin_temp[-1]=np.inf\n",
    "            bin_dict[i]=bin_temp                                        # map into dict      \n",
    "    train_df['CLASS'] = train_df['CLASS'].astype('category')\n",
    "    return train_df,bin_dict\n",
    "\n",
    "def apply_bins(input_df, binning):\n",
    "    test_df = input_df.copy()\n",
    "    #columns = [col for col in list(test_df) if (col != ['ID', 'CLASS'])]\n",
    "    columns = [col for col in list(test_df) if (col != 'ID' and col != 'CLASS')]\n",
    "    columns = [col for col in columns if (test_df[col].dtype in ['float64', 'float32', 'int'])]\n",
    "    \n",
    "    # new version\n",
    "    columns = []\n",
    "    for col in list(test_df):\n",
    "        if(col not in ['ID', 'CLASS'] and test_df[col].dtype in ['float64', 'float32', 'int']):\n",
    "            columns.append(col)\n",
    "    #print(columns)\n",
    "\n",
    "    for i in columns:\n",
    "        binss = binning[i]                                         # get the corresponding bins\n",
    "        labelss = list(range(0,len(binss)-1))                      # labels + 1 = bin values\n",
    "        test_df[i] = pd.cut(test_df[i], bins=binss, labels=labelss, duplicates=\"drop\")\n",
    "        #print(test_df[i].dtypes)                                  # .cut automatically gives categorical data\n",
    "        #test_df[i] = pd.Categorical(test_df[i])                    \n",
    "    return test_df\n",
    "##################################### Below is for testing ####################################################\n",
    "\n",
    "def brier_score(df, correctl):\n",
    "    m,n = df.shape[0],df.shape[1]                                      #  m data with n categories\n",
    "    score = 0\n",
    "    for i in range(m):                              \n",
    "        idx = np.where(df.columns==correctl[i])[0]                # return columns index of corresponding category\n",
    "        corr_binary = [1 if j == idx else 0 for j in range(n)]         # create binary list for this category\n",
    "        temp = sum((corr_binary - df.iloc[i,:])**2)\n",
    "        score = score + temp\n",
    "    score = score/m\n",
    "    return score\n",
    "\n",
    "def accuracy(predictions,correctlabels):\n",
    "    n = predictions.shape[0]\n",
    "    correct = 0\n",
    "    for i in range(n):\n",
    "        df1 = predictions.iloc[i]\n",
    "        pred_label = df1.idxmax()\n",
    "        if (pred_label == correctlabels[i]):\n",
    "            correct += 1\n",
    "    rate = correct/n\n",
    "    return(rate)\n",
    "\n",
    "def label(predictions,correctlabels):\n",
    "    classes = list(predictions)      # return A,B,C\n",
    "    label_dict={}\n",
    "    for c in classes:\n",
    "        label_dict[c] = [1 if temp == c else 0 for temp in correctlabels]\n",
    "    return(label_dict)\n",
    "\n",
    "\n",
    "def binary_auc_dict(prediction, label):       # inputs are one column of prediction and one column of label\n",
    "    score = np.unique(prediction)             # this .unique directly sort the values\n",
    "    score = score[::-1]\n",
    "    sco_dict = dict.fromkeys(score,[0,0])\n",
    "        \n",
    "    for i in range(len(label)):\n",
    "        if label[i] == 1:                     # positive\n",
    "            temp = sco_dict[prediction[i]][0] + 1\n",
    "            sco_dict[prediction[i]] = [temp, sco_dict[prediction[i]][1]]\n",
    "        else:                                 # negtive\n",
    "            temp = sco_dict[prediction[i]][1] + 1\n",
    "            sco_dict[prediction[i]] = [sco_dict[prediction[i]][0], temp]\n",
    "    return sco_dict\n",
    "\n",
    "\n",
    "def binary_auc(tp,fp):\n",
    "    auc = 0\n",
    "    cov_tp = 0\n",
    "    n = len(tp)\n",
    "    if sum(tp)==0:\n",
    "        auc = 0\n",
    "    else:    \n",
    "        for i in range(n):\n",
    "            if fp[i] == 0:\n",
    "                cov_tp +=tp[i]\n",
    "            elif tp[i] == 0:\n",
    "                auc += (cov_tp/sum(tp))*(fp[i]/sum(fp))\n",
    "            else:\n",
    "                auc += (cov_tp/sum(tp))*(fp[i]/sum(fp)) + (tp[i]/sum(tp))*(fp[i]/sum(fp))/2\n",
    "                cov_tp += tp[i]\n",
    "    return(auc)\n",
    "\n",
    "\n",
    "def weighted_auc(auc_dict, correctlabels):\n",
    "    num = len(correctlabels)\n",
    "    labels = [1,2,3,4,5,6,7]\n",
    "    if type(correctlabels) == list:                  # here we assume the testing label is a 'Pandas column' OR a 'list'\n",
    "        correctlabels = correctlabels\n",
    "    else:\n",
    "        correctlabels = correctlabels.tolist()\n",
    "    frequency = {x:correctlabels.count(x)/num for x in correctlabels}      \n",
    "    final_auc = int(0)       \n",
    "    #print(\"Weights:\",frequency)                     # the key types of two dictionaries should be consistent\n",
    "    #dict_keys = auc_dict.keys()\n",
    "    #freq_keys = frequency.keys()\n",
    "    #print(dict_keys)\n",
    "    #print(freq_keys)                       \n",
    "    #print(frequency)\n",
    "    for classes in list(auc_dict):\n",
    "        temp = frequency[classes]*auc_dict[classes][0]\n",
    "        final_auc += temp\n",
    "    return final_auc\n",
    "\n",
    "\n",
    "def auc(predictions,correctlabels):\n",
    "    aaa = label(predictions,correctlabels)\n",
    "    temp = 0\n",
    "    auc_dict={}\n",
    "    for col in predictions.columns:  \n",
    "        bbb = binary_auc_dict(predictions[col],aaa[col])\n",
    "        bbb = pd.DataFrame.from_dict(bbb, orient='index')\n",
    "        bbb.index=range(len(bbb))\n",
    "        pos = bbb.iloc[:,0]\n",
    "        neg = bbb.iloc[:,1]\n",
    "        temp = binary_auc(pos,neg)\n",
    "        auc_dict[col] = [temp]\n",
    "    #print(\" Areas:\",auc_dict)\n",
    "    auc = weighted_auc(auc_dict,correctlabels)\n",
    "    return(auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the class DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class DecisionTree with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, imputatiom, labels, model\n",
    "############################################################################################################################\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# nobins: no. of bins (default = 10)\n",
    "# bintype: either \"equal-width\" (default) or \"equal-size\"\n",
    "# min_samples_split: no. of instances required to allow a split (default = 5)\n",
    "# Output from fit:\n",
    "# nothing      \n",
    "\n",
    "# Hint 2: Define a function, e.g., called divide_and_conquer, that takes the above as input together with df \n",
    "#         and min_samples_split, and also a nodeno (starting with 0) to keep track of the generated nodes in the tree\n",
    "#\n",
    "# Hint 3: You may represent the tree under construction as a list of nodes (tuples), on the form:\n",
    "#         (nodeno,\"leaf\",class_probabilities): corresponding to a leaf node where class_probabilities is a vector\n",
    "#                                              with the relative class frequencies (ordered according to self.labels)\n",
    "#         (nodeno,feature,node_dict): corresponding to an internal (non-leaf) node where node_dict is a mapping from\n",
    "#                                     the possible values of feature to child nodes (their nodenos)\n",
    "#\n",
    "# Hint 4: You may evaluate each feature by a function information_content, which takes the group sizes\n",
    "#         for each possible value of the feature together with the class counts of each group as input\n",
    "#\n",
    "# Hint 5: The best feature found (with lowest residual information content) will be used to split the training\n",
    "#         instances, and each sub-group will be used for generating a sub-tree (recursively by divide_and_conquer,\n",
    "#         see lecture slides for details)\n",
    "#\n",
    "# Hint 6: You may make divide_and_conquer return not only a list of nodes, but also a current_node_no; \n",
    "#         by this, each subsequent call to divide_and_conquer for each subset of instances, i.e. for each feature value, \n",
    "#         could use current_node_no as a starting point.\n",
    "#         If you e.g. make the following call:\n",
    "#\n",
    "#         current_node_no, node_list = divide_and_conquer(current_node_no, ...)\n",
    "#\n",
    "#         then the returned value in current_node_no can be used in the next call to divide_and_conquer.\n",
    "#         Node that node_list will contain an arbitrary number of tuples, each element corresponding to a node together \n",
    "#         with a node number. The first element in the list will have the same number as current_node_no when the call \n",
    "#         was made and the last element will have a number one less than current_node_no when returned, e.g., if there is\n",
    "#         only one (leaf) node in the returned list, then current_node_no will only be incremented by one through the above call.\n",
    "#         \n",
    "# Hint 7: The list of nodes output by divide_and_conquer may finally be converted to an array, where each nodeno in the \n",
    "#         tuples corresponds to an index of the array\n",
    "\n",
    "# The result of applying this function should be:\n",
    "# self.binning should be a discretization mapping (see Assignment 1) from df\n",
    "# self.imputation should be an imputation mapping (see Assignment 1) from df\n",
    "# self.labels should be the categories of the \"CLASS\" column of df, set to be of type \"category\" \n",
    "\n",
    "# self.model should be a decision tree (for details, see lecture slides), where the leafs return class probabilities\n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "# Hint 1: First find the available features (excluding \"CLASS\" and \"ID\"), then find the class counts, e.g., using \n",
    "#         groupby, and calculate the default class probabilities (relative frequencies of the class labels)\n",
    "############################################################################################################################\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.binning=None\n",
    "        self.imputation=None\n",
    "        self.labels=None\n",
    "        self.model=None\n",
    "    \n",
    "    def information_content(self, df):\n",
    "        numEntries = df.shape[0]\n",
    "        label_count = {}\n",
    "        for index, feat in df.iterrows():\n",
    "            currentLabel = int(feat[\"CLASS\"])\n",
    "            if currentLabel not in label_count:\n",
    "                label_count[currentLabel] = 0\n",
    "            label_count[currentLabel] += 1\n",
    "        information_content = 0.0\n",
    "        for label in label_count:\n",
    "            prob = float(label_count[label]) / numEntries\n",
    "            information_content -= prob * np.log2(prob)\n",
    "        #print(information_content[1])\n",
    "        return information_content\n",
    "    \n",
    "    \n",
    "    def get_vector(self, df):\n",
    "        groupSize = df.groupby(\"CLASS\").size()\n",
    "        #groupSize = self.get_group_size(df)\n",
    "        vec = []\n",
    "        for label in self.labels:\n",
    "            vec.append(groupSize[label])\n",
    "        return vec\n",
    "    \n",
    "    def dfSplit(self, df, feature, value):\n",
    "        retdataset = df[df[feature] == value].copy()\n",
    "        del retdataset[feature]\n",
    "        return retdataset\n",
    "    \n",
    "    def BestSplitFeature(self, df, fealist):\n",
    "        baseEntroy = self.information_content(df)\n",
    "        baseInfoGain = 0.0\n",
    "        bestfeature = fealist[0]\n",
    "        for feature in fealist:\n",
    "            features = df[feature]\n",
    "            uniqueVals = set(features)\n",
    "            newEntrypoy = 0.0\n",
    "            for value in uniqueVals:\n",
    "                subdataset = self.dfSplit(df, feature ,value)\n",
    "                prob = len(df[df[feature]==value])/float(len(df))\n",
    "                newEntrypoy += prob*self.information_content(subdataset)\n",
    "            infogain =  newEntrypoy - baseEntroy\n",
    "            if (infogain > baseInfoGain):\n",
    "                baseInfoGain = infogain\n",
    "                bestfeature = feature\n",
    "        return bestfeature \n",
    "    \n",
    "    def divide_and_conquer(self, df, fealist, groupSize, min_sample_split, nodeno):\n",
    "        labelList = list(df[\"CLASS\"].values)\n",
    "        vec = []\n",
    "        for label in self.labels:\n",
    "            vec.append(groupSize[label])\n",
    "        if labelList.count(labelList[0]) == len(labelList):\n",
    "            return (nodeno, \"leaf\", vec)\n",
    "        if fealist is None or len(fealist) == 0:\n",
    "            return (nodeno, \"leaf\", vec)\n",
    "        bestFeature = self.BestSplitFeature(df, fealist)\n",
    "        dit = {}\n",
    "        uniqueVals = df[bestFeature].unique()\n",
    "        feas = fealist.copy()\n",
    "        feas.remove(bestFeature)       \n",
    "        for value in uniqueVals:\n",
    "            sptData = self.dfSplit(df, bestFeature, value)\n",
    "            self.nextNode += 1\n",
    "            dit[value] = self.nextNode\n",
    "            if sptData.shape[0] >= min_sample_split:\n",
    "                group_size_temp = sptData.groupby(\"CLASS\").size()\n",
    "                subtree = self.divide_and_conquer(sptData, feas, group_size_temp, min_sample_split, self.nextNode)\n",
    "            else:\n",
    "                subtree = (self.nextNode, \"leaf\", self.get_vector(sptData))\n",
    "            self.model.append(subtree)\n",
    "        return (nodeno, bestFeature, dit)\n",
    "    \n",
    "    \n",
    "    def fit(self, input_df, nobins = 10, bintype = \"equal-width\", min_samples_split = 5):    #Assign values\n",
    "        df = input_df.copy()\n",
    "        df, self.binning = create_bins(df, nobins, bintype)\n",
    "        df, self.imputation = create_imputation(df)\n",
    "        classes = df['CLASS'].astype('category')     # this is already done in create_functions\n",
    "        self.class_priors = dict(classes.value_counts(normalize = True))\n",
    "        self.labels = classes.cat.categories  \n",
    "        self.model = []\n",
    "        self.nextNode = 0   \n",
    "        \n",
    "        columns = [col for col in list(df) if (col != 'ID' and col != 'CLASS')]\n",
    "        feature_list = []\n",
    "        for feature in columns:\n",
    "                feature_list.append(feature)\n",
    "        class_size = df.groupby(\"CLASS\").size()      # class count\n",
    "        class_freq = class_size/sum(class_size)      # class relative frequency\n",
    "        \n",
    "\n",
    "        #current_node_no, node_list = divide_and_conquer(current_node_no, ...)\n",
    "        # I directly re-visit the current node no inside the divide_and_conquer function\n",
    "        root = self.divide_and_conquer(df, feature_list, class_size, min_samples_split, 0)  \n",
    "        #print(\"root:\",root)\n",
    "        self.model.append(root)\n",
    "        self.model = sorted(self.model, key=lambda x:x[0])\n",
    "        \n",
    "\n",
    "###########################################################################################################################\n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "#              predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "#              are the relative class frequencies in the leaves of the decision tree into which the instances in\n",
    "#              df fall\n",
    "#\n",
    "# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation and binning\n",
    "# Hint 2: Iterate over the rows calling some sub-function, e.g., make_prediction(nodeno,row), which for a test row\n",
    "#         finds a leaf node from which class probabilities are obtained\n",
    "# Hint 3: This sub-function may recursively traverse the tree (represented by an array), starting with the nodeno\n",
    "#         that corresponds to the root\n",
    "############################################################################################################################\n",
    "    def make_prediction(self, nodeno, row):\n",
    "        node = self.model[nodeno]\n",
    "        if node[1] == \"leaf\":\n",
    "            return (node[2])\n",
    "        else:\n",
    "            if row[node[1]] in node[2]:\n",
    "                return self.make_prediction(node[2][row[node[1]]], row)\n",
    "            else:\n",
    "                keys = node[2].keys()\n",
    "                keys = sorted(keys)\n",
    "                if row[node[1]] < keys[0]:\n",
    "                    return self.make_prediction(node[2][keys[0]], row)\n",
    "                for key in keys:\n",
    "                    if key > row[node[1]]:\n",
    "                        return self.make_prediction(node[2][key], row)\n",
    "                return self.make_prediction(node[2][keys[-1]], row)\n",
    "                \n",
    "\n",
    "    def predict(self, df):\n",
    "        columns = [col for col in list(df) if (col != 'ID' and col != 'CLASS')]    \n",
    "        df = df[columns]\n",
    "        df = apply_imputation(df, self.imputation)\n",
    "        df = apply_bins(df, self.binning)\n",
    "        labels = []\n",
    "        for index, row in df.iterrows():\n",
    "            predictions = self.make_prediction(0, row)\n",
    "            labels.append(predictions)\n",
    "        return pd.DataFrame(labels, columns=list(map(int, self.labels)))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (5, 'equal-width', 3): 3.84 s.\n",
      "Testing time (5, 'equal-width', 3): 0.05 s.\n",
      "Training time (5, 'equal-width', 5): 2.91 s.\n",
      "Testing time (5, 'equal-width', 5): 0.04 s.\n",
      "Training time (5, 'equal-width', 10): 1.75 s.\n",
      "Testing time (5, 'equal-width', 10): 0.04 s.\n",
      "Training time (5, 'equal-size', 3): 4.32 s.\n",
      "Testing time (5, 'equal-size', 3): 0.04 s.\n",
      "Training time (5, 'equal-size', 5): 2.35 s.\n",
      "Testing time (5, 'equal-size', 5): 0.04 s.\n",
      "Training time (5, 'equal-size', 10): 1.48 s.\n",
      "Testing time (5, 'equal-size', 10): 0.04 s.\n",
      "Training time (10, 'equal-width', 3): 4.15 s.\n",
      "Testing time (10, 'equal-width', 3): 0.04 s.\n",
      "Training time (10, 'equal-width', 5): 2.60 s.\n",
      "Testing time (10, 'equal-width', 5): 0.05 s.\n",
      "Training time (10, 'equal-width', 10): 2.86 s.\n",
      "Testing time (10, 'equal-width', 10): 0.05 s.\n",
      "Training time (10, 'equal-size', 3): 3.96 s.\n",
      "Testing time (10, 'equal-size', 3): 0.07 s.\n",
      "Training time (10, 'equal-size', 5): 3.39 s.\n",
      "Testing time (10, 'equal-size', 5): 0.05 s.\n",
      "Training time (10, 'equal-size', 10): 3.05 s.\n",
      "Testing time (10, 'equal-size', 10): 0.04 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">5</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-width</th>\n",
       "      <th>3</th>\n",
       "      <td>0.560748</td>\n",
       "      <td>8.869159</td>\n",
       "      <td>0.723693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>9.158879</td>\n",
       "      <td>0.748862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.626168</td>\n",
       "      <td>17.607477</td>\n",
       "      <td>0.746434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-size</th>\n",
       "      <th>3</th>\n",
       "      <td>0.588785</td>\n",
       "      <td>3.121495</td>\n",
       "      <td>0.743689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.635514</td>\n",
       "      <td>4.588785</td>\n",
       "      <td>0.755014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>10.728972</td>\n",
       "      <td>0.697604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"6\" valign=\"top\">10</th>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-width</th>\n",
       "      <th>3</th>\n",
       "      <td>0.644860</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.809237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.598131</td>\n",
       "      <td>5.317757</td>\n",
       "      <td>0.790216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.579439</td>\n",
       "      <td>8.345794</td>\n",
       "      <td>0.748116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">equal-size</th>\n",
       "      <th>3</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>1.429907</td>\n",
       "      <td>0.741858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.570093</td>\n",
       "      <td>1.831776</td>\n",
       "      <td>0.731188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.579439</td>\n",
       "      <td>3.271028</td>\n",
       "      <td>0.735305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Accuracy  Brier score       AUC\n",
       "5  equal-width 3   0.560748     8.869159  0.723693\n",
       "               5   0.607477     9.158879  0.748862\n",
       "               10  0.626168    17.607477  0.746434\n",
       "   equal-size  3   0.588785     3.121495  0.743689\n",
       "               5   0.635514     4.588785  0.755014\n",
       "               10  0.542056    10.728972  0.697604\n",
       "10 equal-width 3   0.644860     4.000000  0.809237\n",
       "               5   0.598131     5.317757  0.790216\n",
       "               10  0.579439     8.345794  0.748116\n",
       "   equal-size  3   0.607477     1.429907  0.741858\n",
       "               5   0.570093     1.831776  0.731188\n",
       "               10  0.579439     3.271028  0.735305"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test your code (leave this part unchanged, except for if auc is undefined)\n",
    "\n",
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "tree_model = DecisionTree()\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "nobins_values = [5,10]\n",
    "bintype_values = [\"equal-width\",\"equal-size\"]\n",
    "min_samples_split_values = [3,5,10]\n",
    "parameters = [(nobins,bintype,min_samples_split) for nobins in nobins_values for bintype in bintype_values \n",
    "              for min_samples_split in min_samples_split_values]\n",
    "\n",
    "results = np.empty((len(parameters),3))\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    t0 = time.perf_counter()\n",
    "    tree_model.fit(glass_train_df,nobins=parameters[i][0],bintype=parameters[i][1],min_samples_split=parameters[i][2])\n",
    "    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = tree_model.predict(glass_test_df)\n",
    "    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=pd.MultiIndex.from_product([nobins_values,bintype_values,min_samples_split_values]),\n",
    "                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.97\n",
      "AUC on training set: 0.99\n",
      "Brier score on training set: 3.68\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "tree_model.fit(glass_train_df,min_samples_split=1)\n",
    "predictions = tree_model.predict(glass_train_df)\n",
    "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the class DecisionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class DecisionForest with three functions __init__, fit and predict (after the comments):\n",
    "#\n",
    "# Input to __init__: \n",
    "# self: the object itself\n",
    "#\n",
    "# Output from __init__:\n",
    "# nothing\n",
    "# \n",
    "# This function does not return anything but just initializes the following attributes of the object (self) to None:\n",
    "# binning, imputatiom, labels, model\n",
    "#\n",
    "# Input to fit:\n",
    "# self: the object itself\n",
    "# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n",
    "# nobins: no. of bins (default = 10)\n",
    "# bintype: either \"equal-width\" (default) or \"equal-size\"\n",
    "# min_samples_split: no. of instances required to allow a split (default = 5)\n",
    "# random_features: no. of features to evaluate at each split (default = 2), 0 means all features (no random sampling)\n",
    "# notrees: no. of trees in the forest (default = 10)\n",
    "#\n",
    "# Output from fit:\n",
    "# nothing\n",
    "#\n",
    "# The result of applying this function should be:\n",
    "#\n",
    "# self.binning should be a discretization mapping (see Assignment 1) from df\n",
    "# self.imputation should be an imputation mapping (see Assignment 1) from df\n",
    "# self.labels should be the categories of the \"CLASS\" column of df, set to be of type \"category\" \n",
    "# self.model should be a random forest (for details, see lecture slides)\n",
    "# Note that the function does not return anything but just assigns values to the attributes of the object.\n",
    "#\n",
    "# Hint 1: Redefine divide_and_conquer to take one additional argument; random_features, and instead of\n",
    "#         evaluating all features choose a random subset, e.g., by np.random.choice (without replacement)\n",
    "# Hint 2: Generate each tree in the forest from a bootstrap replicate of df, e.g., by np.random.choice \n",
    "#         (with replacement) from the index values of df.\n",
    "#\n",
    "class DecisionForest:\n",
    "    def __init__(self):\n",
    "        self.binning=None\n",
    "        self.imputation=None\n",
    "        self.labels=None\n",
    "        self.model=None\n",
    "        \n",
    "    def information_content(self, df):\n",
    "        numEntries = df.shape[0]\n",
    "        labelCounts = {}\n",
    "        for index, feat in df.iterrows():\n",
    "            currentLabel = int(feat[\"CLASS\"])\n",
    "            if currentLabel not in labelCounts:\n",
    "                labelCounts[currentLabel] = 0\n",
    "            labelCounts[currentLabel] += 1\n",
    "        information_content = 0.0\n",
    "        for label in labelCounts:\n",
    "            prob = float(labelCounts[label]) / numEntries\n",
    "            information_content -= prob * np.log2(prob)\n",
    "        return information_content\n",
    "    \n",
    "    def splitDataFrame(self, df, feature, value):\n",
    "        retdataset = df[df[feature] == value].copy()\n",
    "        del retdataset[feature]\n",
    "        return retdataset\n",
    "    \n",
    "    def BestSplitFeature(self, df, features):\n",
    "        baseEntroy = self.information_content(df)\n",
    "        baseInfoGain = 0.0\n",
    "        bestfeature = features[0]\n",
    "        for feature in features:\n",
    "            featlist = df[feature]\n",
    "            uniqueVals = set(featlist)\n",
    "            newEntrypoy = 0.0\n",
    "            for value in uniqueVals:\n",
    "                subdataset = self.splitDataFrame(df, feature ,value)\n",
    "                prob = len(df[df[feature]==value])/float(len(df))\n",
    "                newEntrypoy += prob*self.information_content(subdataset)\n",
    "            infogain =  newEntrypoy - baseEntroy\n",
    "            if (infogain > baseInfoGain):\n",
    "                baseInfoGain = infogain\n",
    "                bestfeature = feature\n",
    "        return bestfeature\n",
    "    \n",
    "    \n",
    "    def get_vector(self, df):\n",
    "        groupSize = df.groupby(\"CLASS\").size()\n",
    "        vec = []\n",
    "        for label in self.labels:\n",
    "            vec.append(groupSize[label])\n",
    "        return vec\n",
    "    \n",
    "    def divide_and_conquer(self, df, features, groupSize, min_sample_split, nodeno):\n",
    "        labelList = list(df[\"CLASS\"].values)\n",
    "        vec = []\n",
    "        for label in self.labels:\n",
    "            vec.append(groupSize[label])\n",
    "        if labelList.count(labelList[0]) == len(labelList):\n",
    "            return (nodeno, \"leaf\", vec)\n",
    "        if features is None or len(features) == 0:\n",
    "            return (nodeno, \"leaf\", vec)\n",
    "        bestFeature = self.BestSplitFeature(df, features)\n",
    "        dit = {}\n",
    "        uniqueVals = df[bestFeature].unique()\n",
    "        feas = features.copy()\n",
    "        feas.remove(bestFeature)        \n",
    "        for value in uniqueVals:\n",
    "            sptData = self.splitDataFrame(df, bestFeature, value)\n",
    "            self.nextNode += 1\n",
    "            dit[value] = self.nextNode\n",
    "            if sptData.shape[0] >= min_sample_split:    \n",
    "                group_size_temp = sptData.groupby(\"CLASS\").size()\n",
    "                subtree = self.divide_and_conquer(sptData, feas, group_size_temp, min_sample_split, self.nextNode)\n",
    "            else:\n",
    "                subtree = (self.nextNode, \"leaf\", self.get_vector(sptData))\n",
    "            self.model.append(subtree)\n",
    "        return (nodeno, bestFeature, dit)\n",
    "\n",
    "\n",
    "    def fit(self, df, nobins=10, bintype=\"equal-width\", min_samples_split=5, random_features=2, notrees=10):\n",
    "\n",
    "        df, self.binning = create_bins(df, nobins, bintype)\n",
    "        df, self.imputation = create_imputation(df)\n",
    "        classes = df['CLASS'].astype('category')     # this is already done in create_functions\n",
    "        self.labels = classes.cat.categories  \n",
    "        self.trees = [] \n",
    "        \n",
    "        columns = [col for col in list(df) if (col != 'ID' and col != 'CLASS')]\n",
    "        feature_list = []\n",
    "        for feature in columns:\n",
    "                feature_list.append(feature)\n",
    "        class_size = df.groupby(\"CLASS\").size()      # class count\n",
    "        class_freq = class_size/sum(class_size)      # class relative frequency\n",
    "             \n",
    "        for i in range(notrees):\n",
    "            if random_features == 0:\n",
    "                self.model = []\n",
    "                self.nextNode = 0\n",
    "                root = self.divide_and_conquer(df, feature_list, class_size, min_samples_split, 0)\n",
    "                self.model.append(root)\n",
    "                self.model = sorted(self.model, key=lambda x:x[0])\n",
    "#               print(model)\n",
    "                self.trees.append(self.model.copy())\n",
    "            else:\n",
    "                feas = np.random.choice(feature_list, random_features, replace=False).tolist()\n",
    "                self.model = []\n",
    "                self.nextNode = 0\n",
    "                root = self.divide_and_conquer(df, feas, class_size, min_samples_split, 0) \n",
    "                self.model.append(root)\n",
    "                self.model = sorted(self.model, key=lambda x:x[0])\n",
    "                self.trees.append(self.model.copy())\n",
    "                \n",
    "# Input to predict:\n",
    "# self: the object itself\n",
    "# df: a dataframe\n",
    "# \n",
    "# Output from predict:\n",
    "# predictions: a dataframe with class labels as column names and the rows corresponding to\n",
    "#              predictions with estimated class probabilities for each row in df, where the class probabilities\n",
    "#              are the mean of all relative class frequencies in the leaves of the forest into which the instances in\n",
    "#              df fall\n",
    "#\n",
    "# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation and binning\n",
    "# Hint 2: Iterate over the rows calling some sub-function, e.g., make_prediction(row), which for a test row\n",
    "#         finds all leaf nodes and calculates the average of their class probabilities\n",
    "    def make_prediction(self, nodeno, row, tree):\n",
    "        node = tree[nodeno]\n",
    "        if node[1] == \"leaf\":\n",
    "            return (node[2])\n",
    "        else:\n",
    "            if row[node[1]] in node[2]:\n",
    "                return self.make_prediction(node[2][row[node[1]]], row, tree)\n",
    "            else:\n",
    "                keys = node[2].keys()\n",
    "                keys = sorted(keys)\n",
    "                if row[node[1]] < keys[0]:\n",
    "                    return self.make_prediction(node[2][keys[0]], row, tree)\n",
    "                for key in keys:\n",
    "                    if key > row[node[1]]:\n",
    "                        return self.make_prediction(node[2][key], row, tree)\n",
    "                return self.make_prediction(node[2][keys[-1]], row, tree)\n",
    "\n",
    "    def predict(self, df):\n",
    "        columns = [col for col in list(df) if (col != 'ID' and col != 'CLASS')]    \n",
    "        df = df[columns]\n",
    "        df = apply_imputation(df, self.imputation)\n",
    "        df = apply_bins(df, self.binning)\n",
    "        labels = []\n",
    "        for index, row in df.iterrows():\n",
    "            curs = []\n",
    "            for tree in self.trees:\n",
    "                probs = self.make_prediction(0, row, tree)\n",
    "                curs.append(probs)\n",
    "            curs = np.array(curs)\n",
    "            curs = np.mean(curs, axis=0)\n",
    "            labels.append(curs)\n",
    "        return pd.DataFrame(labels, columns=list(map(int, self.labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (1, 1): 1.16 s.\n",
      "Testing time (1, 1): 0.06 s.\n",
      "Training time (1, 2): 4.63 s.\n",
      "Testing time (1, 2): 0.15 s.\n",
      "Training time (1, 5): 20.49 s.\n",
      "Testing time (1, 5): 0.13 s.\n",
      "Training time (2, 1): 1.00 s.\n",
      "Testing time (2, 1): 0.05 s.\n",
      "Training time (2, 2): 3.79 s.\n",
      "Testing time (2, 2): 0.08 s.\n",
      "Training time (2, 5): 22.27 s.\n",
      "Testing time (2, 5): 0.11 s.\n",
      "Training time (5, 1): 1.19 s.\n",
      "Testing time (5, 1): 0.08 s.\n",
      "Training time (5, 2): 3.50 s.\n",
      "Testing time (5, 2): 0.08 s.\n",
      "Training time (5, 5): 14.76 s.\n",
      "Testing time (5, 5): 0.10 s.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Brier score</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1</th>\n",
       "      <th>1</th>\n",
       "      <td>0.616822</td>\n",
       "      <td>399.482523</td>\n",
       "      <td>0.681875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.607477</td>\n",
       "      <td>61.677944</td>\n",
       "      <td>0.713227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.682243</td>\n",
       "      <td>3.428972</td>\n",
       "      <td>0.828197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">2</th>\n",
       "      <th>1</th>\n",
       "      <td>0.542056</td>\n",
       "      <td>698.582243</td>\n",
       "      <td>0.656358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.588785</td>\n",
       "      <td>101.406075</td>\n",
       "      <td>0.707183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.691589</td>\n",
       "      <td>3.412243</td>\n",
       "      <td>0.838170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">5</th>\n",
       "      <th>1</th>\n",
       "      <td>0.626168</td>\n",
       "      <td>562.335047</td>\n",
       "      <td>0.688434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.626168</td>\n",
       "      <td>109.784953</td>\n",
       "      <td>0.625709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.644860</td>\n",
       "      <td>4.431682</td>\n",
       "      <td>0.814985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Accuracy  Brier score       AUC\n",
       "1 1  0.616822   399.482523  0.681875\n",
       "  2  0.607477    61.677944  0.713227\n",
       "  5  0.682243     3.428972  0.828197\n",
       "2 1  0.542056   698.582243  0.656358\n",
       "  2  0.588785   101.406075  0.707183\n",
       "  5  0.691589     3.412243  0.838170\n",
       "5 1  0.626168   562.335047  0.688434\n",
       "  2  0.626168   109.784953  0.625709\n",
       "  5  0.644860     4.431682  0.814985"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glass_train_df = pd.read_csv(\"glass_train.txt\")\n",
    "\n",
    "glass_test_df = pd.read_csv(\"glass_test.txt\")\n",
    "\n",
    "forest_model = DecisionForest()\n",
    "\n",
    "test_labels = glass_test_df[\"CLASS\"]\n",
    "\n",
    "min_samples_split_values = [1,2,5]\n",
    "random_features_values = [1,2,5]\n",
    "\n",
    "parameters = [(min_samples_split,random_features) for min_samples_split in min_samples_split_values \n",
    "              for random_features in random_features_values]\n",
    "\n",
    "results = np.empty((len(parameters),3))\n",
    "\n",
    "for i in range(len(parameters)):\n",
    "    t0 = time.perf_counter()\n",
    "    forest_model.fit(glass_train_df,min_samples_split=parameters[i][0],random_features=parameters[i][1])\n",
    "    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    t0 = time.perf_counter()\n",
    "    predictions = forest_model.predict(glass_test_df)\n",
    "    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n",
    "    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n",
    "                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n",
    "\n",
    "results = pd.DataFrame(results,index=pd.MultiIndex.from_product([min_samples_split_values,random_features_values]),\n",
    "                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.65\n",
      "AUC on training set: 0.84\n",
      "Brier score on training set: 57.70\n"
     ]
    }
   ],
   "source": [
    "train_labels = glass_train_df[\"CLASS\"]\n",
    "forest_model.fit(glass_train_df,min_samples_split=1)\n",
    "predictions = forest_model.predict(glass_train_df)\n",
    "print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n",
    "print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n",
    "print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on assumptions, things that do not work properly, etc.\n",
    "We implemented both Decision Tree and Decision Forest for this assignment, the algorithm is quite complex for us, and we have no more time to improve the accuracy. \n",
    "\n",
    "Worth to mention that, we adjusted every assignment with the given datasets on Canvas, and made sure the code can successfully run before submission, which can be referred to out github: https://github.com/liyi201809/Programming-for-Data-Science-ID2214.\n",
    "\n",
    "According to the feedback on the previous assignment, I think our code meets errors mainly because it was tested by some other datasets so some data type conversion cannot work well anymore. I hope this will not influence our grades too much since all of us put a lot efforts into these assignments, and the general algorithms/implementations are in a correct way.\n",
    "\n",
    "Thanks for your reviewing, and we are willing to discuss it further!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
